# In-Context Video Generalist 


<div align="left">
    <a href="https://huggingface.co/feizhengcong/Incontext-Video"><img src="https://img.shields.io/static/v1?label=Models&message=HuggingFace&color=red"></a> &ensp;
    <a href="https://huggingface.co/datasets/feizhengcong/Incontext-Video"><img src="https://img.shields.io/static/v1?label=Dataset&message=HuggingFace&color=blue"></a> &ensp;
    <a href="https://huggingface.co/feizhengcong/Incontext-Video"><img src="https://img.shields.io/static/v1?label=Demo&message=HuggingFace&color=green"></a> &ensp;
</div>

Following image generation in [IC-Lora](https://github.com/ali-vilab/In-Context-LoRA), we directly concatenate both condition and target videos into a single composite video from spacial or time dimension while using natural language to define the task.
It can serve as a general framework for control video generation, with task-specific fine-tuning. 
For more detailed information, please read our technique report. 

## Get Started 


